{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "## Step 0: Make yearly files\n",
    "Data is stored separately for each tile. Combine them to a yearly files"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import h5py as hf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import config as cfg\n",
    "import utils_ls as lut\n",
    "\n",
    "data_path = 'data/ls7/peryear_pertile/'\n",
    "dstination_path = \"data/ls7/after_qa/\"\n",
    "years = cfg.years\n",
    "\n",
    "# load hdfs and csvs\n",
    "\n",
    "hdfs = sorted(glob.glob(data_path+'*.hdf'))\n",
    "\n",
    "csvs = sorted(glob.glob(data_path+'*.csv'))\n",
    "\n",
    "# for each year collect data from different files\n",
    "for year in cfg.years:\n",
    "    # new yearly hdf file\n",
    "    newcsv = dstination_path+'attributes_'+str(year)+'.csv'\n",
    "    newhdf = dstination_path+'barley_'+str(year)+'.hdf'\n",
    "    print(year)\n",
    "\n",
    "    # create new empty yearly-csv\n",
    "    idx_files_to_combine = np.flatnonzero(np.core.defchararray.find(csvs, str(year)) != -1)\n",
    "    new_df = pd.read_csv(csvs[0], nrows=0, index_col=False)\n",
    "    with hf.File(newhdf, 'w') as f:\n",
    "        for idx in idx_files_to_combine:\n",
    "            csvi = csvs[idx]\n",
    "            hdfi = hdfs[idx]\n",
    "\n",
    "            # read attribute df of a tile\n",
    "            df = pd.read_csv(csvi, index_col=False)\n",
    "\n",
    "            # filter df rows based on hdf keys\n",
    "            with hf.File(hdfi, 'r') as rf:\n",
    "                keys = list(rf.keys())\n",
    "            keys_int = np.sort(np.array([int(x) for x in keys]))\n",
    "            df = df[df.new_ID.isin(keys_int)].reset_index()\n",
    "\n",
    "            # read the  dataset from tile-hdf and write to yearly-hdf\n",
    "            with hf.File(hdfi, 'r') as fi:\n",
    "                for key in df.new_ID.values:\n",
    "                    im = fi[str(key)][:]\n",
    "                    f.create_dataset(str(key), data=im, dtype='i2',\n",
    "                                     compression=\"gzip\", compression_opts=9)\n",
    "\n",
    "            new_df = pd.concat([new_df, df], ignore_index=True, sort=False)\n",
    "    new_df.reset_index(inplace=True, drop=True)\n",
    "    new_df[new_df.columns[10:]] = new_df[new_df.columns[10:]].astype(bool)\n",
    "    new_df['year'] = new_df['year'].astype(int)\n",
    "    new_df['area'] = new_df['area'].astype(int)\n",
    "    new_df['loss'] = new_df['loss'].astype(int)\n",
    "    new_df['new_ID'] = new_df['new_ID'].astype(int)\n",
    "    new_df['orig_ID'] = new_df['orig_ID'].astype(str)\n",
    "    if 0:\n",
    "        new_df.to_csv(newcsv, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create median field from yearly files that were created in step 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1315557, 22)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "import h5py as hf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import config as cfg\n",
    "import utils_ls as lut\n",
    "\n",
    "hdfs = sorted(glob.glob('data/ls7/after_qa/*.hdf'))\n",
    "csvs = sorted(glob.glob('data/ls7/after_qa/*.csv'))\n",
    "destination_path = 'data/ls7/median/'\n",
    "\n",
    "nfiles = len(hdfs)\n",
    "for j in np.arange(nfiles):\n",
    "    hdf = hdfs[j]\n",
    "    csv = csvs[j]\n",
    "    year = hdf[-11:-7]\n",
    "    print(f\"year: {year}\", end=\"--->\")\n",
    "    destination_file = destination_path+'barley_median_'+year+'_qa.hdf'\n",
    "    # Load  cloud masked images\n",
    "    images, attrib = lut.load_data(hdf, csv)\n",
    "    # compute median pixel vales of field parcel\n",
    "    medians = lut.compute_median_field(images)\n",
    "\n",
    "    nimages = len(medians)\n",
    "    nbands = medians[0].shape[-1]\n",
    "    with hf.File(destination_file, 'w') as f1:\n",
    "        for i in np.arange(nimages):\n",
    "            key = attrib.new_ID.values[i]\n",
    "            maski = attrib.iloc[i, 10:].values\n",
    "            med = medians[i]\n",
    "            res_med = np.ones((365, nbands))*cfg.fill_value\n",
    "            for k in np.arange(nbands):\n",
    "                res_med[np.where(maski), k] = med[:, k]\n",
    "            f1.create_dataset(str(key),\n",
    "                              data=res_med,\n",
    "                              dtype='f8',\n",
    "                              compression=\"gzip\",\n",
    "                              compression_opts=9)\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "source": [
    "## Step 2: Apply temporal averaging on median fields created in step 1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import h5py as hf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import config as cfg\n",
    "import utils_ls as lut\n",
    "\n",
    "# get list of hdf image files and csv attribute files\n",
    "years = cfg.years\n",
    "attribute_vars = cfg.attribute_vars\n",
    "\n",
    "hdfs = sorted(glob.glob('data/ls7/median/*.hdf'))\n",
    "csvs = sorted(glob.glob('data/ls7/median/*.csv'))\n",
    "nfiles = len(hdfs)\n",
    "print(nfiles)\n",
    "ndvi_index = -1\n",
    "\n",
    "# aggregation parameters\n",
    "day_start = 1\n",
    "day_end = 365\n",
    "day_span = 30\n",
    "window_edges = np.arange(day_start+day_span-1, day_end, day_span)\n",
    "\n",
    "# column names to store the aggregated data\n",
    "ts_cols = ['day ' + str(x)+'-'+str(y) for x, y in\n",
    "           zip(window_edges-day_span, window_edges)]\n",
    "# %%\n",
    "# for each hdf file (which belongs to a year, aggregate the ndvi values)\n",
    "df = pd.DataFrame(columns=attribute_vars+ts_cols)\n",
    "for i in np.arange(nfiles):\n",
    "    year = years[i]\n",
    "    print(year)\n",
    "    # Load hdf and attributes data\n",
    "    median_fields, attrib = lut.load_median_fields(hdfs[i], csvs[i])\n",
    "    median_fields = np.transpose(np.array(median_fields), (2, 1, 0))\n",
    "    vi = median_fields[ndvi_index]\n",
    "    # compute masked mean across time given a matrix (nexamples, nseq)\n",
    "    time_avg_ndvi = np.transpose(lut.window_mean2d(vi, window_edges))\n",
    "    df_temp = pd.DataFrame(time_avg_ndvi, columns=ts_cols)\n",
    "    df_temp[attribute_vars] = attrib[attribute_vars]\n",
    "    df = df.append(df_temp, ignore_index=True, sort=False)\n",
    "destination_file = f'ndvi_{day_start}_{day_span}_{day_end} _all_years.csv'\n",
    "if 0:\n",
    "    df.to_csv(destination_file, index=False)\n",
    "    df = None"
   ]
  },
  {
   "source": [
    "## Create data for area investigation\n",
    "This is carried out only on years with large crop loss fields: 2004, 2008, 2012, 2015"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import h5py as hf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import config as cfg\n",
    "import utils_ls as lut\n",
    "\n",
    "data_path = cfg.data_path + cfg.ml_ready_ndvi_data # fully processed ndvi values\n",
    "fulldf = pd.read_csv(data_path)\n",
    "togverysmall = pd.DataFrame()\n",
    "togsmall = pd.DataFrame()\n",
    "togmedium = pd.DataFrame()\n",
    "toglarge = pd.DataFrame()\n",
    "togverylarge = pd.DataFrame()\n",
    "\n",
    "\n",
    "for year in [2004, 2008, 2012, 2015]:\n",
    "    # print(year)\n",
    "    df = fulldf[fulldf['year'] == year]\n",
    "\n",
    "    verysmalllimit = 50  # 0.5ha\n",
    "    smalllimit = 100  # 0.99 ha\n",
    "    mediumlimit = 300  # 2.99 ha\n",
    "    largelimit = 500  # 4.99ha\n",
    "\n",
    "    small = df[df['area'] < smalllimit]\n",
    "    medium = df[(df['area'] < mediumlimit) & (df['area'] >= smalllimit)]\n",
    "    large = df[df['area'] >= mediumlimit]\n",
    "\n",
    "    print(year)\n",
    "    print('all')\n",
    "    # print('verysmall all: ' +str(verysmall.count()[0]))\n",
    "    print(small.count()[0])\n",
    "    print(medium.count()[0])\n",
    "    print(large.count()[0])\n",
    "    # print(verylarge.count()[0])\n",
    "\n",
    "    print('loss')\n",
    "    # print('verysmall loss: ' + str(np.count_nonzero(verysmall,axis=0)[5]))\n",
    "    print(np.count_nonzero(small, axis=0)[5])\n",
    "    print(np.count_nonzero(medium, axis=0)[5])\n",
    "    print(np.count_nonzero(large, axis=0)[5])\n",
    "\n",
    "    togsmall = pd.concat([togsmall, small])\n",
    "    toglarge = pd.concat([toglarge, large])\n",
    "    togmedium = pd.concat([togmedium, medium])\n",
    "\n",
    "\n",
    "# ndvi_1_30_365_all_years.csv\n",
    "path = 'data/3classes/'\n",
    "name = 'ndvi_1_30_365_all_years_no_ndvieq1_'\n",
    "smallname = path + name + 'small.csv'\n",
    "largename = path + name + 'large.csv'\n",
    "mediumname = path + name + 'medium.csv'\n",
    "\n",
    "\n",
    "togsmall.to_csv(smallname, index=False)\n",
    "toglarge.to_csv(largename, index=False)\n",
    "togmedium.to_csv(mediumname, index=False)\n",
    "# togverysmall.to_csv(verysmallname,index=False)\n",
    "# togverylarge.to_csv(verylargename,index=False)\n",
    "\n",
    "# newlarge = pd.read_csv(cfg.data_path  + name + 'large.csv')\n",
    "\n",
    "# print(np.count_nonzero(newlarge, axis=0)[5])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}